from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import torch
import re
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel
from router_holiday import holiday_router_2025  # â¬…ï¸ ë‹¨ì²´íœ´ë¬´ ë¼ìš°í„° ë¶ˆëŸ¬ì˜¤ê¸°
from card import lawcard_router
import asyncio

# ğŸ”§ ëª¨ë¸ ë¡œë”© (4bit + offload + eval + ì†ë„ ìµœì í™”)
BASE = "beomi/KoAlpaca-Polyglot-5.8B"
ADAPT = "./lora"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16  # ë˜ëŠ” bfloat16ë„ ê°€ëŠ¥
)

print("ğŸ”§ ëª¨ë¸ ë¡œë”© ì¤‘...")


base_model = AutoModelForCausalLM.from_pretrained(
    BASE,
    quantization_config=bnb_config,
    device_map="auto",
    offload_folder="./offload"
)
model = PeftModel.from_pretrained(base_model, ADAPT)
model.eval()  # ì¶”ë¡  ëª¨ë“œ
tokenizer = AutoTokenizer.from_pretrained(BASE)
tokenizer.pad_token = tokenizer.eos_token
print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
print("âœ… ì ìš©ëœ LoRA ëª¨ë“ˆ:", model.modules_to_save)

# FastAPI ì•± êµ¬ì„±
app = FastAPI()

# âœ… CORS í—ˆìš©
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class PromptInput(BaseModel):
    prompt: str
    max_new_tokens: int = 100  # ê¸°ë³¸ê°’ ì¤„ì„
    top_p: float = 0.9
    temperature: float = 0.7

@app.post("/generate")
async def generate_text(data: PromptInput):
    prompt = data.prompt.strip()  # â¬…ï¸ ë¨¼ì € ì„ ì–¸
    print("ğŸ“ ì‚¬ìš©ì ì…ë ¥:", prompt)  # ğŸ‘ˆ ì‚¬ìš©ì ì…ë ¥ ë¡œê·¸

     # âœ… Step 1: ë‹¨ì²´íœ´ë¬´ ë£° ìš°ì„  ì²˜ë¦¬
     
    routed = holiday_router_2025(prompt)
    if routed is not None:
        print("âœ… ë‹¨ì²´íœ´ë¬´ ë£°ë¡œ ì‘ë‹µë¨")
        await asyncio.sleep(2)
        return {"response": routed}

    routed = lawcard_router(prompt)
    if routed is not None:
        print("âœ… ë²•ì¸ì¹´ë“œ ë£°ë¡œ ì‘ë‹µë¨")
        await asyncio.sleep(2)
        return {"response": routed}


    formatted = f"### Instruction:\n{data.prompt.strip()}\n\n### Response:\n"
    inputs = tokenizer(formatted, return_tensors="pt")
    inputs = {k: v.to(model.device) for k, v in inputs.items() if k != "token_type_ids"}

    with torch.inference_mode():
        out = model.generate(
            **inputs,
            max_new_tokens=data.max_new_tokens,
            do_sample=False,
            top_p=data.top_p,
            temperature=data.temperature,
            repetition_penalty=1.05,
            bos_token_id=tokenizer.bos_token_id,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.pad_token_id
        )

    decoded = tokenizer.decode(out[0], skip_special_tokens=True).strip()

    # í›„ì²˜ë¦¬
    if data.prompt.strip() in decoded:
        decoded = decoded.replace(data.prompt.strip(), "").strip()

    decoded = re.sub(r"^#+\s*(ì‘ë‹µ|Response|Instruction)\s*:?[\n]*", "", decoded, flags=re.IGNORECASE)
    decoded = decoded.split("\n###")[0].strip()

    if not decoded or decoded == data.prompt.strip():
        return {"response": "ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆì–´ìš”."}

    return {"response": decoded}