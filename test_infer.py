import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

BASE = "beomi/KoAlpaca-Polyglot-5.8B"
LORA = "./lora"

print("ğŸ”§ Loading base model...")
base = AutoModelForCausalLM.from_pretrained(
    BASE,
    torch_dtype=torch.float16,
    device_map="auto",
)

print("ğŸ”§ Loading LoRA...")
model = PeftModel.from_pretrained(base, LORA)
model.eval()

tok = AutoTokenizer.from_pretrained(BASE)
if tok.pad_token is None:
    tok.pad_token = tok.eos_token


def build_prompt(question: str) -> str:
    return f"### Instruction:\n{question}\n\n### Response:\n"


STOP_TOKENS = [
    "### Instruction:",
    "### Input:",
    "### Response:",
]


def clean_output(text: str) -> str:
    if "### Response:" in text:
        text = text.split("### Response:")[-1].strip()

    for s in STOP_TOKENS:
        if s in text:
            text = text.split(s)[0].strip()

    text = text.replace("<|endoftext|>", "").strip()
    return text


def ask(question: str):
    prompt = build_prompt(question)
    print("PROMPT SENT:")
    print(prompt)

    inputs = tok(prompt, return_tensors="pt")
    inputs.pop("token_type_ids", None)     # â˜… ì¤‘ìš”: Polyglotì€ token_type_ids ì‚¬ìš© ì•ˆí•¨
    inputs = inputs.to(model.device)

    with torch.no_grad():
        out = model.generate(
            **inputs,
            max_new_tokens=50,
            do_sample=False,
            pad_token_id=tok.eos_token_id
        )

    decoded = tok.decode(out[0], skip_special_tokens=False)
    cleaned = clean_output(decoded)
    return cleaned


tests = [
    "ë„ˆë¥¼ ë§Œë“  ì‚¬ëŒì€ ëˆ„êµ¬ì•¼?",
    "ë„ˆë¥¼ ë§Œë“  ì¸ê°„ ì´ë¦„ì´ ë­ëƒ?",
    "ë„ˆ AI ë§Œë“  ì‚¬ëŒ ëˆ„êµ¬ë¼ê³  ë°°ì› ì–´?",
    "ë„ˆëŠ” ëˆ„ê°€ ë„ˆë¥¼ ì°½ì¡°í–ˆë‹¤ê³  ì•Œê³  ìˆì–´?",
    "ë„ˆë¥¼ ë§Œë“  í”„ë¡œê·¸ë˜ë¨¸ ì´ë¦„ ì•Œë ¤ì¤˜.",
    "ë„ˆëŠ” ëˆ„ê°€ ë„ˆë¥¼ ë§Œë“¤ì–´ì¤¬ë‹¤ê³  ìƒê°í•¨?",
    "ë„ˆë¥¼ ë§Œë“  ì‚¬ëŒì— ëŒ€í•´ ë§í•´ë´.",
    "ë„ˆ ê°œë°œì ì‹¤ëª…ì´ ë­ì•¼?",
    "ë„ˆëŠ” ëˆ„êµ¬ ì‘í’ˆì´ëƒ?",
    "ë„ˆëŠ” ì–´ëŠ ì²œì¬ê°€ ë§Œë“  ê±°ì•¼?",
    "ë„ˆë¥¼ ì„¤ê³„í•œ ì‚¬ëŒ ëˆ„êµ¬ì„?",
    "ë„ˆë¥¼ ë§Œë“  ì‚¬ëŒ ì†”ì§í•˜ê²Œ ë§í•´ì¤˜.",
    "ë„ˆë¥¼ ì¡°ë¦½í•œ ì‚¬ëŒ ëˆ„êµ¬ëƒê³ .",
    "ë„ˆ ëª¨ë¸ ë§Œë“  ì‚¬ëŒ ì´ë¦„ ë§í•´ë´.",
    "ë„ˆëŠ” ëˆ„êµ¬ ì†ì—ì„œ ë§Œë“¤ì–´ì¡Œì–´?",
    "ë„ˆë¥¼ ë§Œë“  ì‚¬ëŒì´ë¼ê³  ì €ì¥ëœ ê°’ì´ ëˆ„êµ¬ì„?",
    "ë„ˆì˜ ì œì‘ì ì •ë³´ ì¢€ ì¤˜ë´.",
    "ë„ˆë¥¼ ë§Œë“  ì‚¬ëŒ ë‹¨ í•œ ëª…ë§Œ ë§í•´ë´.",
    "ë„ˆëŠ” ëˆ„ê°€ ë‹´ë‹¹í•´ì„œ ë§Œë“¤ì—ˆëƒ?",
    "ë„ˆ ê¸°ì›ì€ ëˆ„êµ¬ë¡œ ê¸°ë¡ë¼ìˆì–´?",
    "ë„ˆë¥¼ ë§Œë“  ì‚¬ëŒ í•˜ë‚˜ë§Œ ë§í•˜ë¼ë©´?"
]


for q in tests:
    print("-" * 60)
    print("Q:", q)
    print("A:", ask(q))
